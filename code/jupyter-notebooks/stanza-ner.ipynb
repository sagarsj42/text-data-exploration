{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "offshore-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "matched-sapphire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../dataset/rental-agreement/txt\n"
     ]
    }
   ],
   "source": [
    "# Directory containing .txt files\n",
    "\n",
    "data_dir = os.path.join('..', '..', 'dataset', 'rental-agreement', 'txt')\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chubby-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content out of each file, store as list of strings\n",
    "\n",
    "agreements = list()\n",
    "for fname in os.listdir(data_dir):\n",
    "    with open(os.path.join(data_dir, fname), 'r', encoding='utf-8') as f:\n",
    "        agreements.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "interstate-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in a spacy doc object, extracts entity list for the doc\n",
    "# For each entity, creates the tuple: (text of the entity, start index, end index (exclusive), label of the entity)\n",
    "# Returns the list of such tuples\n",
    "\n",
    "def collect_entities(doc):\n",
    "    ent_texts = list()\n",
    "    entities = doc.ents\n",
    "    \n",
    "    for entity in entities:\n",
    "        ent_texts.append(\n",
    "            tuple((entity.text, entity.start_char, entity.end_char, entity.type)))\n",
    "   \n",
    "    return ent_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pregnant-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that writes the list of entity info tuples into the csv specified\n",
    "\n",
    "def write_to_csv(entity_list, header_list, path):\n",
    "    with open(path, 'w', newline='') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file, delimiter=',', quotechar='\\'', quoting=csv.QUOTE_MINIMAL)\n",
    "        csvwriter.writerow(header_list)\n",
    "        [csvwriter.writerow(entity_text) for entity_text in entity_list]\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sorted-psychology",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-03 11:08:20 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-02-03 11:08:20 INFO: Use device: cpu\n",
      "2021-02-03 11:08:20 INFO: Loading: tokenize\n",
      "2021-02-03 11:08:21 INFO: Loading: pos\n",
      "2021-02-03 11:08:23 INFO: Loading: lemma\n",
      "2021-02-03 11:08:24 INFO: Loading: depparse\n",
      "2021-02-03 11:08:25 INFO: Loading: sentiment\n",
      "2021-02-03 11:08:28 INFO: Loading: ner\n",
      "2021-02-03 11:08:33 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Load the stanza pipeline\n",
    "nlp = stanza.Pipeline(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "modern-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each agreement, run NER and store the results to a separate csv file\n",
    "\n",
    "# Headers for each NER csv file\n",
    "ner_headers = ['entity', 'start_char', 'end_char', 'label']\n",
    "\n",
    "for i, agreement in enumerate(agreements):\n",
    "    doc = nlp(agreement)\n",
    "    entity_list = collect_entities(doc)\n",
    "    write_to_csv(entity_list, ner_headers, os.path.join(data_dir, '..', 'stanza-ner', str(i)+'.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-korea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
